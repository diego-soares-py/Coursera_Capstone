{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h1 align=center><font size = 5>Natal Bars's</font></h1>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Introduction\n",
    "\n",
    "This work uses machine learning techniques with a language python and several libraries, associated with the Foursquere API to analyze the location of bars in the city of Natal-RN located in Brazil. Also, you will learn how to use the visualization library, Folium, to visualize the results"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Business Problem\n",
    "We all know that here in Brazil we like parties and bars, in the capital of the state of Rio Grande do Norte called Natal is no different. However, as in all of Brazil, Natal has no growth plan and there are several parts of the city that can be good or bad to open a business, since this neighborhood or region may be saturated with a certain type of business. Returning to the subject of bar, we can see what is the best region or neighborhood to open a new establishment of this type, and in this context we can reach our question for the Data Science journey, “Where can I open a new bar in Natal-RN ? ” The idea here is to look for the neighborhood where there are fewer bars installed, using the aforementioned techniques.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data\n",
    "\n",
    "The data for this project are based on the addresses of the city that are already mentioned, in the case of Natal / RN. The addresses in Brazil, generally, consist of State, City, Neighborhood and finally street address that can be street or avenue, each street address of these may have one or more zip codes. But how do you create those addresses in the city that will mine the data? There is a website that calls a guide but that website contains most of these addresses, because those addresses are on that website in HTML tables. To be able to use this database, we will use scraping techniques on the web in python and save all the content in csv, to have a base with all the hometown parks. Below is the code that is used to consolidate this base."
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "import urllib.request\n",
    "from bs4 import BeautifulSoup\n",
    "import pandas as pd\n",
    "\n",
    "\n",
    "\n",
    "#import geocoder # import geocoder\n",
    "A=[]\n",
    "B=[]\n",
    "C=[]\n",
    "D=[]\n",
    "E=[]\n",
    "\n",
    "i=1\n",
    "\n",
    "while i < 305:\n",
    "    url = \"https://cep.guiamais.com.br/busca/natal-rn?page=\" + str(i)\n",
    "    page = urllib.request.urlopen(url)\n",
    "\n",
    "    soup = BeautifulSoup(page, \"lxml\")\n",
    "    all_tables=soup.find_all(\"table\")\n",
    "    right_table=soup.find('tbody')\n",
    "\n",
    "    for row in right_table.findAll('tr'):    \n",
    "        cells=row.findAll('td')    \n",
    "        B.append(cells[1].find(text=True))\n",
    "        aux = str(cells[2].find(text=True))\n",
    "        aux2 = aux.split(\",\")\n",
    "        C.append(aux2[0])\n",
    "        D.append(aux2[1])\n",
    "        E.append(cells[4].find(text=True))\n",
    "\n",
    "    for row2 in right_table.findAll('tr'):    \n",
    "        cells2=row2.findAll('a')    \n",
    "        #print(len(cells2))\n",
    "        A.append((cells2[0].find(text=True)))\n",
    "        \n",
    "    i = i+1\n",
    "    \n",
    "df=pd.DataFrame(A,columns=['LOGRADOURO'])\n",
    "df['BAIRRO']=B\n",
    "df['CIDADE']=C\n",
    "df['ESTADO']=D\n",
    "df['CEP']=E\n",
    "\n",
    "df.to_csv(r'/root/endereco_natal.csv', index = False, header=True)\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The code above was used to download all content from the site regarding Natal and save it in a csv that we will use in the following steps of our Final project. With the name of the neighborhoods and zip codes in hand we have a perfect base to start mining using the foursquare api and the geopi library, giving us enough tools to answer the question in question from our data project, “Where can I open a new bar in Natal-RN? ”."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
